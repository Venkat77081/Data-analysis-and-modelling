{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates found.\n",
      "Interpolating missing timestamps in column 'Date_of_Creation'...\n",
      "There are outliers in the 'Amount' column: 1104 outliers.\n",
      "Outliers have been treated in the 'Amount' column.\n",
      "No outliers found in the 'Customer_id' column.\n",
      "No outliers found in the 'Salary' column.\n",
      "No outliers found in the 'Account_Number' column.\n",
      "No outliers found in the 'Address_zipcode' column.\n",
      "No outliers found in the 'Recipient_id' column.\n",
      "+----------------+-------------------+----------------+------------------+-------------+-----------+--------------------+-----------+------------------+-----------------+---------------+-------------------+-------+--------------+------+-------------------+---------------+------------+-----------+---------+-------------+\n",
      "|  Transaction_id|   Transaction_time|Transaction_type|            Amount|     Category|       Mode|          Narratives|Customer_id|Customer_firstname|Customer_lastName|Customer_gender|      Date_of_birth| Salary|Account_Number|Status|   Date_of_Creation|Address_zipcode|Recipient_id|      State|     City|       Street|\n",
      "+----------------+-------------------+----------------+------------------+-------------+-----------+--------------------+-----------+------------------+-----------------+---------------+-------------------+-------+--------------+------+-------------------+---------------+------------+-----------+---------+-------------+\n",
      "|eb36e545d2804f8c|2024-03-20 07:06:14|           Debit| 258.1499938964844|Entertainment|Net Banking|       Movie Tickets|1.6099568E7|             Ankit|          Inamdar|           Male|1994-07-07 11:00:00|25000.0|   16099568001|Active|2023-03-19 17:04:37|       560015.0| 85502400001|  Karnataka|Bangalore|     Sun Road|\n",
      "|eb36e545d2804f8c|2024-03-20 07:06:14|          Credit| 258.1499938964844|Entertainment|Net Banking|       Movie Tickets|  8.55024E7|             Aayra|          Chauhan|         Female|1981-06-14 11:00:00|40000.0|   85502400001|Active|2023-03-19 17:05:03|       400092.0| 16099568001|Maharashtra|   Mumbai|Mountain Lane|\n",
      "|be098721c9834d76|2024-03-20 07:06:14|           Debit|251.47000122070312|Entertainment|Net Banking|     Online Shopping|1.6099568E7|             Ankit|          Inamdar|           Male|1994-07-07 11:00:00|25000.0|   16099568001|Active|2023-03-19 17:04:37|       560015.0| 30228785001|  Karnataka|Bangalore|     Sun Road|\n",
      "|be098721c9834d76|2024-03-20 07:06:14|          Credit|251.47000122070312|Entertainment|Net Banking|     Online Shopping|3.0228785E7|           Kuldeep|        Chaudhari|         Female|1991-01-01 11:00:00|40000.0|   30228785001|Active|2023-03-19 17:04:43|       987987.0| 16099568001|    Manipur|      Xyz|      Abcdefg|\n",
      "|f56dff77dba14843|2024-03-20 07:06:14|           Debit|426.94000244140625|       Travel|Net Banking|Public Transport ...|1.6099568E7|             Ankit|          Inamdar|           Male|1994-07-07 11:00:00|25000.0|   16099568001|Active|2023-03-19 17:04:37|       560015.0| 38793037001|  Karnataka|Bangalore|     Sun Road|\n",
      "+----------------+-------------------+----------------+------------------+-------------+-----------+--------------------+-----------+------------------+-----------------+---------------+-------------------+-------+--------------+------+-------------------+---------------+------------+-----------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- Transaction_id: string (nullable = true)\n",
      " |-- Transaction_time: timestamp (nullable = true)\n",
      " |-- Transaction_type: string (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Mode: string (nullable = true)\n",
      " |-- Narratives: string (nullable = true)\n",
      " |-- Customer_id: double (nullable = true)\n",
      " |-- Customer_firstname: string (nullable = true)\n",
      " |-- Customer_lastName: string (nullable = true)\n",
      " |-- Customer_gender: string (nullable = true)\n",
      " |-- Date_of_birth: timestamp (nullable = true)\n",
      " |-- Salary: double (nullable = true)\n",
      " |-- Account_Number: long (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      " |-- Date_of_Creation: timestamp (nullable = true)\n",
      " |-- Address_zipcode: double (nullable = true)\n",
      " |-- Recipient_id: long (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Street: string (nullable = true)\n",
      "\n",
      "No missing values in the dataset.\n",
      "Table 'Customers' is successfully created in Hive.\n",
      "The 'Customers' table contains 656 records.\n",
      "Table 'Accounts' is successfully created in Hive.\n",
      "The 'Accounts' table contains 691 records.\n",
      "Table 'Transactions' is successfully created in Hive.\n",
      "The 'Transactions' table contains 102292 records.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace, to_timestamp, from_utc_timestamp, initcap, when, concat_ws, split, sum, min as pyspark_min, stddev, mean\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType, LongType, TimestampType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "\n",
    "#-----------------------Cleansing and Transformation pipeline----------------------# \n",
    "\n",
    "def load_data_spark(spark, file_path, encoding='utf-8'):\n",
    "    # Define the schema for your DataFrame\n",
    "    schema = StructType([\n",
    "        StructField('Transaction_id', StringType()),\n",
    "        StructField('Transaction_time', StringType()),\n",
    "        StructField('Transaction_type', StringType()),\n",
    "        StructField('Amount', FloatType()),\n",
    "        StructField('Category', StringType()),\n",
    "        StructField('Mode', StringType()),\n",
    "        StructField('Narratives', StringType()),\n",
    "        StructField('Customer_id', IntegerType()),\n",
    "        StructField('Customer_firstname', StringType()),\n",
    "        StructField('Customer_lastName', StringType()),\n",
    "        StructField('Customer_gender', StringType()),\n",
    "        StructField('Date_of_birth', StringType()),\n",
    "        StructField('Salary', FloatType()),\n",
    "        StructField('Account_Number', LongType()),\n",
    "        StructField('Status', StringType()),\n",
    "        StructField('Date_of_Creation', StringType()),\n",
    "        StructField('Address_zipcode', IntegerType()),  # Use StringType for flexible formatting\n",
    "        StructField('Address_state_address_City_address_Street', StringType()),\n",
    "        StructField('Recipient_id', StringType()),\n",
    "        StructField('Duplicate', StringType())\n",
    "    ])\n",
    "    \n",
    "    # Load data into DataFrame using defined schema and header\n",
    "    df = spark.read.csv(file_path, header=False, schema=schema, encoding=encoding)\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_duplicates(df):\n",
    "    # Drop duplicates based on all columns\n",
    "    df = df.dropDuplicates()\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_address(df):\n",
    "    # Define a function to clean the address column\n",
    "    clean_address_expr = (\n",
    "        regexp_replace(col('Address_state_address_City_address_Street'), r'[^a-zA-Z0-9,\\s-]', '')\n",
    "    )\n",
    "    \n",
    "    # Replace consecutive dashes at the beginning with a single dash\n",
    "    clean_address_expr = (\n",
    "        regexp_replace(clean_address_expr, r'^(-)+', '-')  # Replace multiple leading dashes with a single dash\n",
    "    )\n",
    "    \n",
    "    # Apply the cleaning expression to the DataFrame\n",
    "    cleaned_df = df.withColumn('Address_state_address_City_address_Street', clean_address_expr)\n",
    "    return cleaned_df\n",
    "\n",
    "\n",
    "def transform_data(df):\n",
    "    # Step 1: Create a new column for 'recipient_id_str' containing only string values\n",
    "    df = df.withColumn('Recipient_id_str', regexp_replace(col('Recipient_id'), '\\\\d+', ''))\n",
    "\n",
    "    # Step 2: Concatenate 'address_state_address_City_address_Street' with 'recipient_id_str'\n",
    "    df = df.withColumn('New_address', concat_ws('', col('Address_state_address_City_address_Street'), col('Recipient_id_str')))\n",
    "\n",
    "    # Step 3: Drop the duplicate of the 'recipient_id' column\n",
    "    df = df.drop('Recipient_id_str', 'Address_state_address_City_address_Street')\n",
    "\n",
    "    # Step 4: Remove string values from 'recipient_id' column (keeping only numeric values)\n",
    "    df = df.withColumn('Recipient_id', regexp_replace(col('Recipient_id'), '\\\\D+', ''))\n",
    "\n",
    "    # Step 5: Cast 'Duplicate' column to match 'Recipient_id' column's type\n",
    "    df = df.withColumn('Duplicate', col('Duplicate').cast(df.schema['Recipient_id'].dataType))\n",
    "\n",
    "    # Step 6: Replace null values in 'recipient_id' with corresponding values from the 'duplicate' column\n",
    "    df = df.withColumn('Recipient_id', when(col('Recipient_id').isNull() | (col('Recipient_id') == ''), col('Duplicate')).otherwise(col('Recipient_id')))\n",
    "\n",
    "    # Step 7: Convert 'Recipient_id' to LongType\n",
    "    df = df.withColumn('Recipient_id', df['Recipient_id'].cast(LongType()))\n",
    "\n",
    "    # Step 8: Drop the 'duplicate' column\n",
    "    df = df.drop('Duplicate')\n",
    "    \n",
    "    # Step 9: Split 'new_address' into separate columns for 'State', 'City', and 'Street'\n",
    "    df = df.withColumn('State', split(col('New_address'), '-').getItem(0))\n",
    "    df = df.withColumn('City', split(col('New_address'), '-').getItem(1))\n",
    "    df = df.withColumn('Street', split(col('New_address'), '-').getItem(2))\n",
    "\n",
    "    # Step 10: Drop the 'new_address' column\n",
    "    df = df.drop('New_address')\n",
    "    \n",
    "    # Step 11: Define a list of non-standard representations of null values to replace\n",
    "    non_standard_nulls = ['null', 'Null', 'na', '']\n",
    "\n",
    "    # Step 12: Iterate over each column in the DataFrame\n",
    "    for column in df.columns:\n",
    "        # Replace non-standard null representations with None\n",
    "        df = df.withColumn(column, when(col(column).isin(non_standard_nulls), None).otherwise(col(column)))\n",
    "\n",
    "        # Convert empty strings to None (treat them as null)\n",
    "        df = df.withColumn(column, when(col(column) == '', None).otherwise(col(column)))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def data_type(df):    \n",
    "    # Step 1: Convert 'Date_of_birth' to TimestampType with timezone (Asia/Kolkata)\n",
    "    df = df.withColumn('Date_of_birth', from_utc_timestamp(to_timestamp(col('Date_of_birth'), 'dd-MM-yyyy HH:mm:ss'), 'Asia/Kolkata'))\n",
    "\n",
    "    # Step 2: Convert 'Transaction_time' to TimestampType with timezone (Asia/Kolkata)\n",
    "    df = df.withColumn('Transaction_time', from_utc_timestamp(to_timestamp(col('Transaction_time'), 'dd-MM-yyyy HH:mm:ss'), 'Asia/Kolkata'))\n",
    "\n",
    "    # Step 3: Convert 'Date_of_Creation' to TimestampType with timezone (Asia/Kolkata)\n",
    "    df = df.withColumn('Date_of_Creation', from_utc_timestamp(to_timestamp(col('Date_of_Creation'), 'yyyy-MM-dd\\'T\\'HH:mm:ss'), 'Asia/Kolkata'))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def encoding(df):    \n",
    "    # Step 1: Replace values in 'customer_gender' column\n",
    "    df = df.withColumn('Customer_gender', when(col('Customer_gender') == 'F', 'Female')\n",
    "                                          .when(col('Customer_gender') == 'M', 'Male')\n",
    "                                          .otherwise(col('Customer_gender')))\n",
    "    return df\n",
    "\n",
    "\n",
    "def capitalize_string_columns(df, excluded_columns=['Transaction_id']):\n",
    "    # Identify string columns in the DataFrame excluding the specified columns\n",
    "    string_columns = [col_name for col_name, dtype in df.dtypes if dtype == 'string' and col_name not in excluded_columns]\n",
    "\n",
    "    # Apply initcap function to each string column\n",
    "    for col_name in string_columns:\n",
    "        df = df.withColumn(col_name, initcap(col(col_name)))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def fill_missing_values(df):\n",
    "    # Fill missing values with mode (most frequent value) for string columns\n",
    "    string_columns = [col_name for col_name, dtype in df.dtypes if dtype == 'string']\n",
    "    for col_name in string_columns:\n",
    "        mode_value = df.groupBy(col_name).count().orderBy(col('count').desc()).select(col_name).first()[0]\n",
    "        df = df.withColumn(col_name, when(col(col_name).isNull(), mode_value).otherwise(col(col_name)))\n",
    "    \n",
    "    # Fill missing values with mean for numeric columns\n",
    "    numeric_columns = [col_name for col_name, dtype in df.dtypes if dtype in ('int', 'long', 'float')]\n",
    "    for col_name in numeric_columns:\n",
    "        mean_value = df.select(mean(col_name)).collect()[0][0]\n",
    "        df = df.withColumn(col_name, when(col(col_name).isNull(), mean_value).otherwise(col(col_name)))\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "def fill_missing_timestamps_with_oldest_value(df):\n",
    "    from pyspark.sql import Window\n",
    "    from pyspark.sql import functions as F\n",
    "    \n",
    "    # Identify timestamp columns in the DataFrame with missing values\n",
    "    timestamp_columns_with_missing_values = [\n",
    "        col_name for col_name, dtype in df.dtypes\n",
    "        if dtype == 'timestamp' and df.filter(col(col_name).isNull()).count() > 0\n",
    "    ]\n",
    "\n",
    "    # If no timestamp columns with missing values are found, return the original DataFrame\n",
    "    if not timestamp_columns_with_missing_values:\n",
    "        return df\n",
    "\n",
    "    # Iterate over each timestamp column with missing values and perform interpolation\n",
    "    for timestamp_column in timestamp_columns_with_missing_values:\n",
    "        # Find the oldest (minimum) valid timestamp value in the column\n",
    "        oldest_timestamp = df.select(F.min(col(timestamp_column))).first()[0]\n",
    "\n",
    "        # Log or print the details of the interpolation process\n",
    "        print(f\"Interpolating missing timestamps in column '{timestamp_column}'...\")\n",
    "\n",
    "        # Apply linear interpolation using window function to fill missing timestamps\n",
    "        interpolated_column = (\n",
    "            F.when(col(timestamp_column).isNull(),\n",
    "                   F.last(timestamp_column, ignorenulls=True).over(Window.orderBy('Transaction_time').rowsBetween(Window.unboundedPreceding, 0)))\n",
    "            .otherwise(col(timestamp_column))\n",
    "        )\n",
    "\n",
    "        # Fill remaining missing values with the oldest valid timestamp\n",
    "        df = df.withColumn(timestamp_column, F.when(col(timestamp_column).isNull(), oldest_timestamp).otherwise(col(timestamp_column)))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def identify_and_treat_outliers(df):\n",
    "    # Get numeric column names\n",
    "    numeric_columns = [col_name for col_name, dtype in df.dtypes if dtype in ('int', 'bigint', 'float', 'double')]\n",
    "    \n",
    "    # Iterate over each numeric column to identify and treat outliers\n",
    "    for column in numeric_columns:\n",
    "        # Calculate mean and standard deviation for the column\n",
    "        column_mean = df.select(mean(col(column))).first()[0]\n",
    "        column_stddev = df.select(stddev(col(column))).first()[0]\n",
    "        \n",
    "        # Set outlier bounds based on mean and standard deviation\n",
    "        lower_bound = column_mean - 3 * column_stddev\n",
    "        upper_bound = column_mean + 3 * column_stddev\n",
    "        \n",
    "        # Identify outliers in the column\n",
    "        outliers = df.filter((col(column) < lower_bound) | (col(column) > upper_bound))\n",
    "        outliers_count = outliers.count()\n",
    "        \n",
    "        if outliers_count > 0:\n",
    "            print(f\"There are outliers in the '{column}' column: {outliers_count} outliers.\")\n",
    "            \n",
    "            # Treat outliers by capping them at the lower or upper bound\n",
    "            df = df.withColumn(column, when(col(column) < lower_bound, lower_bound).when(col(column) > upper_bound, upper_bound).otherwise(col(column)))\n",
    "            print(f\"Outliers have been treated in the '{column}' column.\")\n",
    "        else:\n",
    "            print(f\"No outliers found in the '{column}' column.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_to_hadoop(df, hdfs_path):\n",
    "    # Write DataFrame to HDFS in CSV format with a single output file\n",
    "    df.write.mode(\"overwrite\").csv(hdfs_path)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "def main_spark():\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"DataPreprocessing\").getOrCreate()\n",
    "\n",
    "    # Load the data\n",
    "    file_path = 'file:///home/susan1232/savingtransactions 3.csv'\n",
    "    data = load_data_spark(spark, file_path, encoding='utf-8')\n",
    "    \n",
    "    # Check for duplicates\n",
    "    if data.count() == data.dropDuplicates().count():\n",
    "        print(\"No duplicates found.\")\n",
    "    else:\n",
    "        print(\"Duplicates are present.\")\n",
    "\n",
    "        # Remove duplicates\n",
    "        data = remove_duplicates(data)\n",
    "        \n",
    "        # Print confirmation message after removing duplicates\n",
    "        print(\"Duplicates have been removed.\")\n",
    "\n",
    "    # Clean up the address column\n",
    "    data = clean_address(data)\n",
    "    \n",
    "    # Transform the data\n",
    "    data = transform_data(data)\n",
    "    \n",
    "    # Correct data types\n",
    "    data = data_type(data)\n",
    "    \n",
    "    # Encode gender values\n",
    "    data = encoding(data)\n",
    "    \n",
    "    # Capitalize column values\n",
    "    data = capitalize_string_columns(data)\n",
    "    \n",
    "    # Fill missing values based on column types\n",
    "    data = fill_missing_values(data)\n",
    "    \n",
    "    # Fill missing timestamps\n",
    "    data = fill_missing_timestamps_with_oldest_value(data)\n",
    "    \n",
    "    # Identify and treat outliers in numeric columns\n",
    "    data=identify_and_treat_outliers(data)\n",
    "    \n",
    "          \n",
    "  \n",
    "    # Define the HDFS directory where you want to save the CSV file\n",
    "    hdfs_path = \"hdfs://nameservice1/user/susan1232/Livestream/data.csv\"\n",
    "\n",
    "    # Save DataFrame to HDFS\n",
    "    save_to_hadoop(data, hdfs_path)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Validation and Visualization\n",
    "    \n",
    "    data.show(5)  # Show the first 5 rows of the cleaned data\n",
    "    data.printSchema()  # Print the schema of the cleaned DataFrame\n",
    "    \n",
    "    # Count the number of null values in each column\n",
    "    missing_counts = data.select([sum(col(column).isNull().cast('int')).alias(column) for column in data.columns])\n",
    "\n",
    "    # Collect the missing value counts into a dictionary\n",
    "    missing_value_counts = missing_counts.collect()[0].asDict()\n",
    "\n",
    "    # Identify columns with missing values\n",
    "    columns_with_missing_values = [column for column, count in missing_value_counts.items() if count > 0]\n",
    "\n",
    "    # Print columns with missing values and their respective counts\n",
    "    if columns_with_missing_values:\n",
    "          for column in columns_with_missing_values:\n",
    "                print(f\"Column '{column}' has {missing_value_counts[column]} missing values.\")\n",
    "    else:\n",
    "        print(\"No missing values in the dataset.\")    \n",
    "            \n",
    "   \n",
    "    # Stop the Spark session\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_spark()\n",
    "    \n",
    "    \n",
    "#-----------------------Hadoop to Hive Tables----------------------# \n",
    "\n",
    "def create_and_load_tables(spark, input_path, database_name):\n",
    "    # Define schema for the CSV file\n",
    "    schema = \"Transaction_id STRING,Transaction_time TIMESTAMP,Transaction_type STRING,Amount DOUBLE,Category STRING,Mode STRING,Narratives STRING,Customer_id DOUBLE,Customer_firstname STRING,Customer_lastName STRING,Customer_gender STRING,Date_of_birth TIMESTAMP,Salary DOUBLE,Account_Number BIGINT,Status STRING,Date_of_Creation TIMESTAMP,Address_zipcode STRING,Recipient_id BIGINT,State STRING,City STRING,Street STRING\"\n",
    "\n",
    "    # Read CSV file into DataFrame\n",
    "    df = spark.read.csv(input_path, header=False, schema=schema)\n",
    "\n",
    "    # Extract and clean data for Customers table\n",
    "    customers_df = df.select(\"Customer_id\", \"Customer_firstname\", \"Customer_lastName\", \"Customer_gender\", \"Date_of_birth\", \"Salary\", \"State\", \"City\", \"Street\", \"Address_zipcode\").dropDuplicates()\n",
    "\n",
    "    # Extract and clean data for Accounts table\n",
    "    accounts_df = df.select(\"Customer_id\", \"Account_Number\", \"Status\", \"Date_of_Creation\").dropDuplicates()\n",
    "\n",
    "    # Extract and clean data for Transactions table\n",
    "    transactions_df = df.select(\"Account_Number\", \"Transaction_id\", \"Transaction_time\", \"Transaction_type\", \"Mode\", \"Category\", \"Narratives\", \"Recipient_id\").dropDuplicates()\n",
    "\n",
    "    # Set the current database context\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "    spark.sql(f\"USE {database_name}\")\n",
    "\n",
    "    # Define table names\n",
    "    table_names = {\n",
    "        \"Customers\": customers_df,\n",
    "        \"Accounts\": accounts_df,\n",
    "        \"Transactions\": transactions_df\n",
    "    }\n",
    "\n",
    "    # Create tables and load data\n",
    "    for table_name, table_df in table_names.items():\n",
    "        table_df.createOrReplaceTempView(f\"tmp_{table_name}\")\n",
    "        spark.sql(f\"CREATE TABLE IF NOT EXISTS {table_name} AS SELECT * FROM tmp_{table_name}\")\n",
    "\n",
    "        # Verify table creation\n",
    "        result = spark.sql(f\"SELECT COUNT(*) AS total_records FROM {table_name}\")\n",
    "        total_records = result.first().total_records\n",
    "\n",
    "        if total_records > 0:\n",
    "            print(f\"Table '{table_name}' is successfully created in Hive.\")\n",
    "            print(f\"The '{table_name}' table contains {total_records} records.\")\n",
    "        else:\n",
    "            print(f\"The '{table_name}' table is empty.\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"create_and_load_tables_pipeline\") \\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://ip-10-1-2-24.ap-south-1.compute.internal:9083\") \\\n",
    "        .config(\"spark.sql.warehouse.dir\", \"hdfs://nameservice1/user/hive/warehouse\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Define Hadoop HDFS path where the CSV file is stored\n",
    "    hadoop_path = \"hdfs://nameservice1/user/susan1232/Livestream/data.csv\"\n",
    "\n",
    "    # Define database name\n",
    "    database_name = \"RetailBanking\"\n",
    "\n",
    "    # Call the function to create and load tables\n",
    "    create_and_load_tables(spark, hadoop_path, database_name)\n",
    "\n",
    "    # Stop Spark session\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
