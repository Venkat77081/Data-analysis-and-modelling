{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d94d658f",
   "metadata": {},
   "source": [
    "### Install the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35de463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install datasets\n",
    "!pip install sagemaker_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fefe8c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import datetime\n",
    "import time\n",
    "import tarfile\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.session import Session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3653b1dd",
   "metadata": {},
   "source": [
    "### Establishing connection between notebook and sagemaker and creating session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9d48172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-1\n"
     ]
    }
   ],
   "source": [
    "sm_boto3 = boto3.client(\"sagemaker\")\n",
    "sess = sagemaker.Session()\n",
    "my_region = sess.boto_session.region_name\n",
    "print(my_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d1ede4",
   "metadata": {},
   "source": [
    "### Creating S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a51a6180",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'testcaseclassification1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03729571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bucket testcaseclassification1 successfully created\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "try:\n",
    "    if my_region == 'us-east-1':\n",
    "        s3.create_bucket(Bucket=bucket_name)\n",
    "        print(f\"The bucket {bucket_name} successfully created\")\n",
    "except Exception as e:\n",
    "    print('S3 error')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ad74cd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://testcaseclassification1/distilbertmodel/output\n"
     ]
    }
   ],
   "source": [
    "output_path='s3://{}/{}/output'.format(bucket_name, prefix)\n",
    "print(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725b5f4e",
   "metadata": {},
   "source": [
    "### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cda6c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel(\"Deposit - Unified_output.xlsx\")\n",
    "df.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c229c788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scenario Description</th>\n",
       "      <th>Function</th>\n",
       "      <th>Sub-Function</th>\n",
       "      <th>Feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scenario to Validate  one month Mudaraba depos...</td>\n",
       "      <td>DEPOSIT</td>\n",
       "      <td>Creation</td>\n",
       "      <td>Deposit Creation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scenario to Validate  one month Mudaraba depos...</td>\n",
       "      <td>DEPOSIT</td>\n",
       "      <td>Authorization</td>\n",
       "      <td>Deposit Authorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scenario to validate created one month Mudarab...</td>\n",
       "      <td>DEPOSIT</td>\n",
       "      <td>Enquiry</td>\n",
       "      <td>Accounting Entries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scenario to Validate  one USD month Mudaraba d...</td>\n",
       "      <td>DEPOSIT</td>\n",
       "      <td>Creation</td>\n",
       "      <td>Deposit Creation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Scenario to Validate  one USD month Mudaraba d...</td>\n",
       "      <td>DEPOSIT</td>\n",
       "      <td>Authorization</td>\n",
       "      <td>Deposit Authorization</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Scenario Description Function   Sub-Function  \\\n",
       "0  Scenario to Validate  one month Mudaraba depos...  DEPOSIT       Creation   \n",
       "1  Scenario to Validate  one month Mudaraba depos...  DEPOSIT  Authorization   \n",
       "2  Scenario to validate created one month Mudarab...  DEPOSIT        Enquiry   \n",
       "3  Scenario to Validate  one USD month Mudaraba d...  DEPOSIT       Creation   \n",
       "4  Scenario to Validate  one USD month Mudaraba d...  DEPOSIT  Authorization   \n",
       "\n",
       "                 Feature  \n",
       "0       Deposit Creation  \n",
       "1  Deposit Authorization  \n",
       "2     Accounting Entries  \n",
       "3       Deposit Creation  \n",
       "4  Deposit Authorization  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_columns = ['Scenario Description', 'Function', 'Sub-Function', 'Feature']\n",
    "df = df[selected_columns]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b08d53",
   "metadata": {},
   "source": [
    "### Spliting Dataset into train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "092a5003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(819, 4) (351, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data= np.split(df.sample(frac=1, random_state=1729), [int(0.7*len(df))])\n",
    "print(train_data.shape, test_data.shape)\n",
    "\n",
    "train_data.to_csv('train.csv', index=False)\n",
    "test_data.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859df563",
   "metadata": {},
   "source": [
    "### Saving Train and Test dataset into S3 buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "335c14a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'distilbertmodel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c662019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://testcaseclassification1/distilbertmodel/train/train.csv'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainpath = sess.upload_data(\n",
    "    path=\"train.csv\", bucket=bucket_name, key_prefix=f\"{prefix}/train\"\n",
    ")\n",
    "\n",
    "trainpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "769cd0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://testcaseclassification1/distilbertmodel/test/test.csv'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testpath = sess.upload_data(\n",
    "    path=\"test.csv\", bucket=bucket_name, key_prefix=f\"{prefix}/test\"\n",
    ")\n",
    "\n",
    "testpath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46b1597",
   "metadata": {},
   "source": [
    "### Create Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "609b209d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing distilbert_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile distilbert_script.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "import boto3\n",
    "\n",
    "\n",
    "# Custom Dataset class\n",
    "class ScenarioFunctionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.encodings = tokenizer(\n",
    "            texts.tolist(),\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=256  # Adjust based on your specific needs and resources\n",
    "        )\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return item\n",
    "\n",
    "# Load the model\n",
    "def model_fn(model_dir):\n",
    "    model_path = os.path.join(model_dir, \"model.joblib\")\n",
    "    model = joblib.load(model_path)\n",
    "    \n",
    "    # Load the tokenizer\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model_dir)\n",
    "    \n",
    "    # Load the label encoder\n",
    "    label_encoder_path = os.path.join(model_dir, \"label_encoder.joblib\")\n",
    "    label_encoder = joblib.load(label_encoder_path)\n",
    "    \n",
    "    return model, tokenizer, label_encoder\n",
    "\n",
    "def decode_predictions(predictions, label_encoder):\n",
    "    decoded_predictions = label_encoder.inverse_transform(predictions)\n",
    "    return decoded_predictions\n",
    "\n",
    "def predict_fn(input_data, model_and_tokenizer):\n",
    "    model, tokenizer, label_encoder = model_and_tokenizer\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize the input data\n",
    "    encodings = tokenizer(\n",
    "        input_data,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Perform prediction\n",
    "    with torch.no_grad():\n",
    "        input_ids = encodings['input_ids']\n",
    "        attention_mask = encodings['attention_mask']\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = (logits > 0.0).float().cpu().numpy()\n",
    "    \n",
    "    # Decode the predictions\n",
    "    decoded_predictions = decode_predictions(predictions, label_encoder)\n",
    "    return decoded_predictions\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"[INFO] Extracting arguments\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    parser.add_argument(\"--epochs\", type=int, default=3)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=5e-5)\n",
    "    parser.add_argument(\"--train_batch_size\", type=int, default=8)\n",
    "    parser.add_argument(\"--eval_batch_size\", type=int, default=8)\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "    parser.add_argument(\"--train\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\"))\n",
    "    parser.add_argument(\"--test\", type=str, default=os.environ.get(\"SM_CHANNEL_TEST\"))\n",
    "    parser.add_argument(\"--train-file\", type=str, default=\"train.csv\")\n",
    "    parser.add_argument(\"--test-file\", type=str, default=\"test.csv\")\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    print(\"[INFO] Reading data\")\n",
    "    \n",
    "    train_df = pd.read_csv(os.path.join(args.train, args.train_file))\n",
    "    test_df = pd.read_csv(os.path.join(args.test, args.test_file))\n",
    "\n",
    "    X_train = train_df['Scenario Description'].astype(str)\n",
    "    y_train = train_df[['Function', 'Sub-Function', 'Feature']]\n",
    "\n",
    "    X_test = test_df['Scenario Description'].astype(str)\n",
    "    y_test = test_df[['Function', 'Sub-Function', 'Feature']]\n",
    "\n",
    "    all_classes = set(y_train['Function']).union(set(y_train['Sub-Function'])).union(set(y_train['Feature']))\n",
    "    y_train_combined = y_train.apply(lambda x: tuple(all_classes & set(x)), axis=1)\n",
    "    y_test_combined = y_test.apply(lambda x: tuple(all_classes & set(x)), axis=1)\n",
    "\n",
    "    # Fit and transform the label encoder on your training data\n",
    "    label_encoder = MultiLabelBinarizer(classes=list(all_classes))\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train_combined)\n",
    "    y_test_encoded = label_encoder.fit_transform(y_test_combined)\n",
    "    \n",
    "    # Save the label encoder to a file\n",
    "    label_encoder_path = os.path.join(args.model_dir, \"label_encoder.joblib\")\n",
    "    joblib.dump(label_encoder, label_encoder_path)\n",
    "    \n",
    "\n",
    "    model_name = 'distilbert-base-uncased'\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "    num_labels = len(all_classes)\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "    train_dataset = ScenarioFunctionDataset(X_train, y_train_encoded, tokenizer)\n",
    "    test_dataset = ScenarioFunctionDataset(X_test, y_test_encoded, tokenizer)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.train_batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.eval_batch_size, shuffle=False)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=args.learning_rate)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    num_epochs = args.epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        average_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}: Average training loss = {average_train_loss}\")\n",
    "\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predictions = (logits > 0.0).float()\n",
    "\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Save the model using joblib\n",
    "    model_path = os.path.join(args.model_dir, \"model.joblib\")\n",
    "    joblib.dump(model, model_path)\n",
    "    print(\"Model persisted at \" + model_path)\n",
    "\n",
    "    # Save the tokenizer\n",
    "    tokenizer.save_pretrained(args.model_dir)\n",
    "\n",
    "# Example input data\n",
    "input_data = [\"Example scenario description\"]\n",
    "\n",
    "# Make a prediction\n",
    "prediction = predict_fn(input_data, model_fn(args.model_dir))\n",
    "print(\"Prediction:\", prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbab1d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    # Decode and predict\n",
    "    sample_texts = [\"Sample scenario description 1\", \"Sample scenario description 2\"]\n",
    "    encodings = tokenizer(sample_texts, padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "    encodings = {key: val.to(device) for key, val in encodings.items()}\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings)\n",
    "        logits = outputs.logits\n",
    "        predictions = (logits > 0.0).float().cpu().numpy()\n",
    "\n",
    "    decoded_predictions = decode_predictions(predictions, label_encoder)\n",
    "    print(\"Predictions:\", decoded_predictions)\n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4972bb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Extracting arguments\n",
      "[INFO] Reading data\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "Epoch 1: Average training loss = 0.2693900925297182\n",
      "Epoch 2: Average training loss = 0.10753095309132511\n",
      "Epoch 3: Average training loss = 0.08089174966788987\n",
      "Epoch 4: Average training loss = 0.06017997576990752\n",
      "Epoch 5: Average training loss = 0.047650689819773426\n",
      "Test Accuracy: 0.4815\n",
      "Model persisted at ./model.joblib\n"
     ]
    }
   ],
   "source": [
    "! python distilbert_script.py --epochs 5 \\\n",
    "                              --learning_rate 5e-5 \\\n",
    "                              --train_batch_size 8 \\\n",
    "                              --eval_batch_size 8 \\\n",
    "                              --model-dir ./ \\\n",
    "                              --train ./ \\\n",
    "                              --test ./ \\\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eacb17",
   "metadata": {},
   "source": [
    "### Run model in sagemaker environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06733f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-471112636257\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2024-06-23-17-01-25-755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-23 17:01:26 Starting - Starting the training job...\n",
      "2024-06-23 17:01:45 Starting - Preparing the instances for training...\n",
      "2024-06-23 17:02:16 Downloading - Downloading input data......\n",
      "2024-06-23 17:03:01 Downloading - Downloading the training image...\n",
      "2024-06-23 17:03:52 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-06-23 17:03:54,883 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-06-23 17:03:54,886 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-06-23 17:03:54,894 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-06-23 17:03:54,896 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-06-23 17:03:55,068 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.1/9.1 MB 89.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.23.0\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 402.6/402.6 kB 63.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\u001b[0m\n",
      "\u001b[34mDownloading regex-2024.5.15-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (776 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 776.2/776.2 kB 66.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 1)) (1.22.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 1)) (21.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 1)) (4.64.0)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.20,>=0.19\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.19.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 90.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 1)) (2.28.1)\u001b[0m\n",
      "\u001b[34mCollecting filelock\u001b[0m\n",
      "\u001b[34mDownloading filelock-3.15.4-py3-none-any.whl (16 kB)\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.4.1\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 74.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting fsspec>=2023.5.0\u001b[0m\n",
      "\u001b[34mDownloading fsspec-2024.6.0-py3-none-any.whl (176 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 176.9/176.9 kB 27.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers->-r requirements.txt (line 1)) (4.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers->-r requirements.txt (line 1)) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2022.6.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers->-r requirements.txt (line 1)) (1.26.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers->-r requirements.txt (line 1)) (3.3)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: safetensors, regex, fsspec, filelock, huggingface-hub, tokenizers, transformers\u001b[0m\n",
      "\u001b[34mAttempting uninstall: fsspec\u001b[0m\n",
      "\u001b[34mFound existing installation: fsspec 2022.7.1\u001b[0m\n",
      "\u001b[34mUninstalling fsspec-2022.7.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled fsspec-2022.7.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed filelock-3.15.4 fsspec-2024.6.0 huggingface-hub-0.23.4 regex-2024.5.15 safetensors-0.4.3 tokenizers-0.19.1 transformers-4.41.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 24.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-06-23 17:04:06,400 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-06-23 17:04:06,400 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-06-23 17:04:06,404 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-06-23 17:04:06,415 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-06-23 17:04:06,427 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-06-23 17:04:06,435 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.m5.large\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 5,\n",
      "        \"eval_batch_size\": 8,\n",
      "        \"learning_rate\": 5e-05,\n",
      "        \"train_batch_size\": 8\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.m5.large\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"pytorch-training-2024-06-23-17-01-25-755\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-471112636257/pytorch-training-2024-06-23-17-01-25-755/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"distilbert_script\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 2,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.large\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.large\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"distilbert_script.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":5,\"eval_batch_size\":8,\"learning_rate\":5e-05,\"train_batch_size\":8}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=distilbert_script.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.m5.large\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=distilbert_script\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=2\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-471112636257/pytorch-training-2024-06-23-17-01-25-755/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.m5.large\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":5,\"eval_batch_size\":8,\"learning_rate\":5e-05,\"train_batch_size\":8},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"pytorch-training-2024-06-23-17-01-25-755\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-471112636257/pytorch-training-2024-06-23-17-01-25-755/source/sourcedir.tar.gz\",\"module_name\":\"distilbert_script\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"distilbert_script.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"5\",\"--eval_batch_size\",\"8\",\"--learning_rate\",\"5e-05\",\"--train_batch_size\",\"8\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=5\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=5e-05\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 distilbert_script.py --epochs 5 --eval_batch_size 8 --learning_rate 5e-05 --train_batch_size 8\u001b[0m\n",
      "\u001b[34m[INFO] Extracting arguments\u001b[0m\n",
      "\u001b[34m[INFO] Reading data\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[2024-06-23 17:04:10.658 algo-1:40 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[2024-06-23 17:04:10.829 algo-1:40 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2024-06-23 17:04:10.830 algo-1:40 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2024-06-23 17:04:10.831 algo-1:40 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2024-06-23 17:04:10.831 algo-1:40 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2024-06-23 17:04:10.832 algo-1:40 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mEpoch 1: Average training loss = 0.27019670475455165\u001b[0m\n",
      "\u001b[34mEpoch 2: Average training loss = 0.107440489299089\u001b[0m\n",
      "\u001b[34mEpoch 3: Average training loss = 0.07753458365942668\u001b[0m\n",
      "\u001b[34mEpoch 4: Average training loss = 0.05578676280249091\u001b[0m\n",
      "\u001b[34mEpoch 5: Average training loss = 0.04634648577753201\u001b[0m\n",
      "\n",
      "2024-06-23 17:19:10 Uploading - Uploading generated training model\u001b[34mTest Accuracy: 0.4815\u001b[0m\n",
      "\u001b[34mModel persisted at /opt/ml/model/model.joblib\u001b[0m\n",
      "\u001b[34mPrediction: [('Enquiry', 'DEPOSIT')]\u001b[0m\n",
      "\u001b[34m2024-06-23 17:19:04,999 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-06-23 17:19:05,000 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-06-23 17:19:05,000 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-06-23 17:19:38 Completed - Training job completed\n",
      "Training seconds: 1042\n",
      "Billable seconds: 1042\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Get the execution role\n",
    "role = get_execution_role()\n",
    "\n",
    "# Define the Estimator\n",
    "estimator = PyTorch(\n",
    "    entry_point='distilbert_script.py',\n",
    "    role=role,\n",
    "    framework_version='1.12.0',  # specify the PyTorch version you want to use\n",
    "    py_version='py38',\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",  # choose an instance type with a GPU\n",
    "    hyperparameters={\n",
    "        'epochs': 5,\n",
    "        'learning_rate': 5e-5,\n",
    "        'train_batch_size': 8,\n",
    "        'eval_batch_size': 8\n",
    "    },\n",
    "    dependencies=['requirements.txt']\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "# Launch the training job\n",
    "estimator.fit({'train': trainpath, 'test': testpath})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "548098d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model data saved at: s3://sagemaker-us-east-1-471112636257/pytorch-training-2024-06-23-17-01-25-755/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "model_data_path = estimator.model_data\n",
    "print(\"Model data saved at:\", model_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1cbdfbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Assuming 'model_data' is the S3 location of your trained model artifacts\n",
    "# Replace 'role' with your SageMaker execution role ARN\n",
    "model = PyTorchModel(model_data=model_data_path,\n",
    "                     role=get_execution_role(),\n",
    "                     entry_point='distilbert_script.py',\n",
    "                     framework_version='1.8.1',\n",
    "                     py_version='py3')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e85558aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Repacking model artifact (s3://sagemaker-us-east-1-471112636257/pytorch-training-2024-06-23-17-01-25-755/output/model.tar.gz), script artifact (None), and dependencies ([]) into single tar.gz file located at s3://sagemaker-us-east-1-471112636257/pytorch-inference-2024-06-23-18-00-03-634/model.tar.gz. This may take some time depending on model size...\n",
      "INFO:sagemaker:Creating model with name: pytorch-inference-2024-06-23-18-00-29-961\n",
      "INFO:sagemaker:Creating endpoint-config with name test1\n",
      "INFO:sagemaker:Creating endpoint with name test1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "# Deploy the model to an endpoint\n",
    "predictor = model.deploy(initial_instance_count=1,\n",
    "                         instance_type='ml.m5.xlarge',\n",
    "                         endpoint_name='test1',\n",
    "                         serializer=sagemaker.serializers.JSONSerializer(),\n",
    "                         deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    "                         # Increase timeout to 300 seconds (adjust as needed)\n",
    "                         model_name='distilbert',\n",
    "                         wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a14fc0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "predictor = Predictor(endpoint_name='test1',\n",
    "                      sagemaker_session=sagemaker.Session(),\n",
    "                      serializer=JSONSerializer(),\n",
    "                      deserializer=JSONDeserializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "32142aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Status: InService\n"
     ]
    }
   ],
   "source": [
    "# Describe the endpoint to get its status\n",
    "endpoint_description = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "\n",
    "print(\"Endpoint Status:\", endpoint_description['EndpointStatus'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ede6b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "input_data = [\"Example scenario description\"]\n",
    "\n",
    "# Serialize input data to JSON\n",
    "payload = json.dumps(input_data)\n",
    "\n",
    "# Make prediction\n",
    "response = predictor.predict(payload)\n",
    "\n",
    "print(\"Prediction:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a260db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "input_data = [\"Example scenario description\"]\n",
    "\n",
    "# Serialize input data to JSON\n",
    "payload = json.dumps(input_data)\n",
    "\n",
    "\n",
    "try:\n",
    "    # Make prediction\n",
    "    response = predictor.predict(payload)\n",
    "    print(\"Prediction:\", response)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Prediction error:\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0108db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# Assuming you have already deployed an endpoint named 'test'\n",
    "\n",
    "predictor = Predictor(endpoint_name='test',\n",
    "                      sagemaker_session=sagemaker.Session())\n",
    "\n",
    "# Set up a JSON serializer and deserializer\n",
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = JSONDeserializer()\n",
    "\n",
    "# Input data for prediction\n",
    "input_data = [\"Example scenario description\"]\n",
    "\n",
    "try:\n",
    "    # Make prediction\n",
    "    response = predictor.predict(input_data)\n",
    "\n",
    "    print(\"Prediction:\", response)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Prediction error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1f9977",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(predictor.endpoint)\n",
    "bucket_to_delete = boto3.resource('s3').Bucket(bucket_name)\n",
    "bucket_to_delete.objects.all().delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3a123e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
